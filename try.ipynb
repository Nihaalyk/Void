{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Document Agentic RAG for Quantum Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Groq API Key\n",
    "\n",
    "Ensure your Groq API key is stored in the environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Groq client\n",
    "from groq import Groq\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SimpleDirectoryReader' from 'llama_index' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     SimpleDirectoryReader,\n\u001b[1;32m      5\u001b[0m     SummaryIndex,\n\u001b[1;32m      6\u001b[0m     VectorStoreIndex,\n\u001b[1;32m      7\u001b[0m     ObjectIndex\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionCallingAgentWorker, AgentRunner\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenTextSplitter\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SimpleDirectoryReader' from 'llama_index' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Updated Imports\n",
    "\n",
    "# Example: Adjust these paths based on the actual structure of the package\n",
    "from llama_index.readers import SimpleDirectoryReader\n",
    "from llama_index.indexes import SummaryIndex, VectorStoreIndex, ObjectIndex\n",
    "from llama_index.agents import FunctionCallingAgentWorker, AgentRunner\n",
    "from llama_index.parsers import TokenTextSplitter\n",
    "from llama_index.tools import QueryEngineTool, FunctionTool\n",
    "from llama_index.llms import LLM, ChatMessage, MessageRole\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "def get_vector_tool(nodes: List, algo: str, embed_model: any):\n",
    "    '''Creates a vector index tool that performs vector search.'''\n",
    "    \n",
    "    vector_index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
    "    \n",
    "    def vector_query(query: str) -> str:\n",
    "        \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "        query (str): the string query to be embedded.\n",
    "        \"\"\"\n",
    "        query_engine = vector_index.as_query_engine(similarity_top_k=4)\n",
    "        response = query_engine.query(query)\n",
    "        return str(response)\n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"{algo}_vector_tool\",\n",
    "        fn=vector_query\n",
    "    )\n",
    "\n",
    "    return vector_query_tool\n",
    "\n",
    "def get_summary_tool(nodes: List, algo: str, llm: any, embed_model: any):\n",
    "    '''Creates a summary index tool that performs summarization.'''\n",
    "    \n",
    "    summary_index = SummaryIndex(nodes, embed_model=embed_model)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "        llm=llm\n",
    "    )\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        name=f\"{algo}_summary_tool\",\n",
    "        query_engine=summary_query_engine,\n",
    "        description=(\n",
    "            f\"Useful if you want to get a summary or explanation of {algo}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return summary_tool\n",
    "\n",
    "def get_tools(documents: List, llm: any, embed_model: any) -> List:\n",
    "    '''Returns vector index and summary index tools for the provided documents.'''\n",
    "\n",
    "    tools = []\n",
    "    for document in documents:\n",
    "        # Get the document name\n",
    "        doc_name = Path(document.metadata.get('filename', 'document')).stem\n",
    "\n",
    "        # Split text into chunks\n",
    "        splitter = TokenTextSplitter(\n",
    "            chunk_size=64,\n",
    "            chunk_overlap=10,\n",
    "            separator=\" \",\n",
    "        )\n",
    "        nodes = splitter.get_nodes_from_documents([document])\n",
    "        if len(nodes) <= 1:\n",
    "            raise ValueError(f'Number of generated nodes is less than or equal to 1. Check the document {document}')\n",
    "\n",
    "        # Get summary and vector index tool\n",
    "        vector_tool = get_vector_tool(nodes=nodes, algo=doc_name, embed_model=embed_model)\n",
    "        summary_tool = get_summary_tool(nodes=nodes, algo=doc_name, llm=llm, embed_model=embed_model)\n",
    "\n",
    "        # Store tools\n",
    "        tools.extend([vector_tool, summary_tool])\n",
    "\n",
    "    return tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from the PDF folder\n",
    "\n",
    "documents = SimpleDirectoryReader('path_to_your_pdf_folder').load_data()\n",
    "\n",
    "# Define the GroqLLM class\n",
    "from llama_index.llms.base import LLM, ChatMessage, MessageRole\n",
    "from typing import List\n",
    "\n",
    "class GroqLLM(LLM):\n",
    "    def __init__(self, client, model_name, temperature=1.0, max_tokens=1024, top_p=1.0):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.top_p = top_p\n",
    "\n",
    "    def chat(self, messages: List[ChatMessage]) -> str:\n",
    "        # Convert ChatMessages to the format expected by Groq client\n",
    "        groq_messages = []\n",
    "        for message in messages:\n",
    "            if message.role == MessageRole.USER:\n",
    "                groq_messages.append({'role': 'user', 'content': message.content})\n",
    "            elif message.role == MessageRole.SYSTEM:\n",
    "                groq_messages.append({'role': 'system', 'content': message.content})\n",
    "            elif message.role == MessageRole.ASSISTANT:\n",
    "                groq_messages.append({'role': 'assistant', 'content': message.content})\n",
    "        \n",
    "        # Call the Groq client\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=groq_messages,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            top_p=self.top_p,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "        # Return the assistant's reply\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "# Initialize the LLM\n",
    "model_name = \"llama3-8b-8192\"\n",
    "llm = GroqLLM(client, model_name)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# Get the tools\n",
    "tools = get_tools(documents=documents, llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object index and retriever for tools\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")\n",
    "obj_retriever = obj_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query agent\n",
    "response = agent.query(\n",
    "    \"Tell me about Quantum Circuits\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Building Agentic RAG with LlamaIndex](https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
